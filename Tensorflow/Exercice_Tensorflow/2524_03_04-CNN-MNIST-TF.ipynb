{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2524_03_04-CNN-MNIST-TF.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.8.3 64-bit (conda)","metadata":{"interpreter":{"hash":"425de0034fe2a5cccb37fa718577d50d422d932bcdd34bd9c458ed6f91cc55c9"}}},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ylqHZTrTiR0i","colab_type":"text"},"cell_type":"markdown","source":["#  Convolutional neural network\n","# Reconnaissance Optique de Caractères avec TensorFlow\n","\n","## Objectif :\n","Nous allons voir comment apprendre à reconnaître des caractères optiques avec TensorFlow.\n","\n","(OCR : Optical Caracter Recognition)\n","\n","Il s'agit de concevoir un modèle de Deep Learning afin de reconnaître des chiffres traçés à la main.\n","\n","Ce type d'application est déployé pour la lecture optique des chèques bancaires par exemple.\n","\n","Nous allons écrire un Réseau de neurones à Convolution afin de reconnaître les caractères manuscrits.\n","\n","C'est un problème de classification multiple (10 chiffres possible => 10 classes)"]},{"metadata":{"id":"7wzZ7PfLPwqC","colab_type":"code","colab":{}},"cell_type":"code","source":["# Network Parameters\n","num_input = 784 # MNIST data input (img shape: 28*28)\n","num_classes = 10 # MNIST total classes (chiffres manuscrits de 0-9)\n","dropout = 0.25 # Dropout, probabilité de drop un neurone du réseau"],"execution_count":2,"outputs":[]},{"metadata":{"id":"DNXwG3r2jGIw","colab_type":"text"},"cell_type":"markdown","source":["## Importation des librairies \n","On importe les données nécessaires depuis TensorFlow. TensorFlow embarque un ensemble de data-sets."]},{"metadata":{"id":"Q55g7ye1gnif","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"7fd0edc9-b06a-44ee-d267-503c08c80253","executionInfo":{"status":"ok","timestamp":1553596548141,"user_tz":-60,"elapsed":1211,"user":{"displayName":"bardol franck","photoUrl":"https://lh4.googleusercontent.com/-JNlTS3TmFkQ/AAAAAAAAAAI/AAAAAAAAIEA/fJiA10muy2o/s64/photo.jpg","userId":"04651379096066137536"}}},"cell_type":"code","source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","# fix verbosity\n","old_v = tf.logging.get_verbosity()\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","from tensorflow.examples.tutorials.mnist import input_data\n","mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /Users/laminediamban/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow.examples.tutorials'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-8770e02723fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST_data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.examples.tutorials'"]}]},{"metadata":{"id":"o99FIDENZFW3","colab_type":"text"},"cell_type":"markdown","source":["## Visualiser le data-set"]},{"metadata":{"id":"bzDxDUysTy-M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2c9a08e2-ec4a-4b81-aac2-9a483cb92cf4","executionInfo":{"status":"ok","timestamp":1553596557833,"user_tz":-60,"elapsed":1608,"user":{"displayName":"bardol franck","photoUrl":"https://lh4.googleusercontent.com/-JNlTS3TmFkQ/AAAAAAAAAAI/AAAAAAAAIEA/fJiA10muy2o/s64/photo.jpg","userId":"04651379096066137536"}}},"cell_type":"code","source":["import matplotlib.pyplot as pl\n","\n","print mnist.train.images.shape"],"execution_count":3,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"Missing parentheses in call to 'print'. Did you mean print(mnist.train.images.shape)? (<ipython-input-3-c6201bad31eb>, line 3)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-c6201bad31eb>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print mnist.train.images.shape\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(mnist.train.images.shape)?\n"]}]},{"metadata":{"id":"WcOrZgBOaU4L","colab_type":"text"},"cell_type":"markdown","source":["### Redimension :\n","On choisit une image du datat-set.\n","\n","On passe d'une dimension *784* à *28 x 28* pour pouvoir afficher l'image choisit"]},{"metadata":{"id":"9R6j4Vg9Zi9o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"4c91a50f-0e2c-4696-d27d-02618963f53e","executionInfo":{"status":"ok","timestamp":1553596576222,"user_tz":-60,"elapsed":1074,"user":{"displayName":"bardol franck","photoUrl":"https://lh4.googleusercontent.com/-JNlTS3TmFkQ/AAAAAAAAAAI/AAAAAAAAIEA/fJiA10muy2o/s64/photo.jpg","userId":"04651379096066137536"}}},"cell_type":"code","source":["img_nb = 14\n","img  = mnist.train.images[img_nb]\n","\n","# avant re dimension\n","print(img.shape)\n","\n","# 784 -> 28 x 28\n","img = mnist.train.images[img_nb].reshape(28,28)\n","\n","# après re dimension\n","print(img.shape)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["(784,)\n","(28, 28)\n"],"name":"stdout"}]},{"metadata":{"id":"lwWYtEmZaYZH","colab_type":"text"},"cell_type":"markdown","source":["### Plot"]},{"metadata":{"id":"HlYJ204zaZkT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":364},"outputId":"c70cf049-7b9a-4320-b9ef-a55e18318d45","executionInfo":{"status":"ok","timestamp":1553596584512,"user_tz":-60,"elapsed":876,"user":{"displayName":"bardol franck","photoUrl":"https://lh4.googleusercontent.com/-JNlTS3TmFkQ/AAAAAAAAAAI/AAAAAAAAIEA/fJiA10muy2o/s64/photo.jpg","userId":"04651379096066137536"}}},"cell_type":"code","source":["pl.imshow(img ,cmap = pl.get_cmap('gray'))"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fb786a836d0>"]},"metadata":{"tags":[]},"execution_count":20},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGBAWAKAAWEJAAaEJQAYEJYAYPBfkOAvMM0r4m8AAAAASUVORK5CYII=\n","text/plain":["<Figure size 576x396 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"_QUvYE364GQ7","colab_type":"text"},"cell_type":"markdown","source":["## Input layer : les placeHolders\n","Les  noeud nommés **placeholder** ne réalisent **aucun** calcul. Ils sont chargé d'alimenter, d'emettre des données à l'exécution (*run time*).\n","\n","Dimension du placeholder attendu par TF  :\n","```\n"," [batch_size, image_height, image_width, channels]\n","```\n","\n"," -1 pour  batch size : cette dimension est calculee à la volee (*runtime*).\n"," \n","Ceci permet d'effectuer des *batch* dont la dimension sera basée sur le nombre de valeur d entrée *(features[\"x\"])*\n"]},{"metadata":{"id":"l_5nQq7Q4ZBE","colab_type":"code","colab":{}},"cell_type":"code","source":["x = tf.placeholder(\"float\", shape=[None, num_input])\n","y_ = tf.placeholder(\"float\", shape=[None, num_classes])\n","# image 28 x 28 ; N&B <=> channel = 1\n","\n","x_image = tf.reshape(x, [-1,28,28,1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Pvgw07Kq4gy9","colab_type":"text"},"cell_type":"markdown","source":["## Fonctions de construction \n","Nous déportons dans des fonctions les opérations \n","* initialisation des variables et des constantes\n","* construction des couches de convolution\n","* construction des couches de Pooling"]},{"metadata":{"id":"wNWGdMBIhQ1d","colab_type":"code","colab":{}},"cell_type":"code","source":["# poids et constantes dans des fonctions (+lisible) \n","def weight_variable(shape):\n","  initial = tf.truncated_normal(shape, stddev=0.1)\n","  return tf.Variable(initial)\n","\n","def bias_variable(shape):\n","  initial = tf.constant(0.1, shape=shape)\n","  return tf.Variable(initial)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bnNz7s-q48po","colab_type":"text"},"cell_type":"markdown","source":["## Architecture du CNN \n","\n","Le réseau que l'on va construire ensemble :\n","```\n","input :\n","image : 784 pixels (28 x 28) \n","Réseau : \n","couche_conv#1\n"," [CONV - ReLu - Pool] \n","couche_conv#2\n"," [CONV - ReLu - Pool] \n"," couche_dense#1\n"," [DENSE - Drop-out] \n"," couche_decision\n"," [DENSE - softmax] \n","\n","output :\n","probabilité appartenance à une classe {0,1,2,...,9}\n","```\n","\n","\n","\n","L'architecture du réseau, les différentes couches sont les suivantes :\n","* couche convolution  #1:\n","\n","applique 32 filtres 5x5  , activation ReLu\n","* couche Pooling  #1: \n","\n","applique *max pooling* filtre 2x2  stride = 2 (*no overlap*)\n","* couche convolution #2: \n","\n","applique 64 filtres 5x5, ReLU \n","* couche Pooling  #2: \n","\n","idem pooling#1\n","* couche dense  #1: \n","\n","1,024 neurones, régularisation *dropout*, (probabilité élagage durant apprentissage)\n","* couche dense #2 (couche Logits): 10 neurones, un pour chaque classe de chiffres manuscrits (nombre entre 0 et9).\n","\n","\n","\n"]},{"metadata":{"id":"Q21BUElDN2-8","colab_type":"text"},"cell_type":"markdown","source":["## Détails de construction des couches de Pooling, convolution et dense\n","Cette partie technique est à passer en 1ère lecture\n","### Convolution\n","* stride de 1 dans chaque dimension ```\n","strides = [1, 1, 1, 1] ```\n","\n","\n","* zero padding : \"SAME\"\n","\n","Ce type de *padding* préserve la dimension de l'image retournée\n","\n","``` \n","  tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","```\n","\n","Souvent, on ne souhaite pas effectuer un *downsampling* trop prématuré. Dans ce cas, on choisit de préserver à l'aide d'un *padding* approprié.\n","\n","### Pooling\n","L'opération de *pooling* qui est implémenté est une opération \"max\". \n","C'est donc une couche *max_pool*\n","\n","fenêtre 2 x 2, *stride* de 2 \n","(pas de chevauchement : *over-lapp*) puis calcul du maximum au sein de chaque fenetre\n","```\n","tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')\n","```\n","### Couche dense \n","La couche dense est souvent nommé : * fully connected layer*\n","\n","C'est la couche chargée d'effectuer la classification des caractéristiques (*features*) \n","* extraits par les couches de convolution \n","* réduits par les couches de *pooling*\n","\n","La couche finale *W_fc2* contient une cellule (*node*) pour chaque classe à prédire (10 classes)\n","\n","\n"]},{"metadata":{"id":"0qtTgXsoDsJe","colab_type":"text"},"cell_type":"markdown","source":["## Fonctions de construction des couches\n","* input : tenseur (*tensor*)\n","* output : tenseur\n","\n","### Chaînage des couches\n","Les couches sont liées entre elles de la façon suivante: \n","\n","L'output de la couche précédente servira d'*input* à la couche suivante"]},{"metadata":{"id":"UGlBKdHAzygd","colab_type":"code","colab":{}},"cell_type":"code","source":["# les operations au sein du reseau CNN (convolution et max pool)\n","def conv2d(x, W):\n","  #stride de 1 dans chaque dimension (zero padding : \"SAME\") \n"," return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n","\n","def max_pool_2x2(x):\n","  \n","  # zero padding model : preserve la dimension  (SAME)\n","  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n","                      strides=[1, 2, 2, 1], padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aEgC4qMaATGh","colab_type":"text"},"cell_type":"markdown","source":["## Couche convolution #1\n","* **inputs** : images 28 x 28\n","* Transformation : convolution 5 x 5 \n","* **output** : 32 *feature maps*\n","* activation *reLu*\n","* max-pool 2 x 2\n","\n","Lien de connection : *h_pool1*\n","\n","\n","Important pour comprendre le détail des mécanismes de la convolution mais **à passer en 1ère lecture** :\n","\n","Dimension *h_pool1*  (tenseur)\n","```\n"," [batch_size,height,width,channels] = [-1, 14, 14, 32]\n","\n","```\n","L'opération de **Pooling**, *max_pool_2x2*, reduit la dimension (*down-sampling*) de 50% des images d'origine.\n","En conséquence, \n","\n","taille 28 x 28 -> max Pool -> taille 14 x 14"]},{"metadata":{"id":"tH_ueCkxwRxt","colab_type":"code","colab":{}},"cell_type":"code","source":["# dimension du tensor : [filter_size , filter_size , channel , #features #filtres (output dim)]\n","# filter_size : 5 x 5\n","filter_size = 5\n","# sur le couche de convolution#1, channel = 1 (image N&B) \n","channel = 1\n","nb_features = 32\n","W_conv1 = weight_variable([filter_size, filter_size, channel, nb_features])\n","# 1 biais pour chaque matrice de poids (nb_features)\n","b_conv1 = bias_variable([nb_features])\n","h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n","# max-pooling (conservation du maximum calculé sur des fenêtres 2x2) \n","h_pool1 = max_pool_2x2(h_conv1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d129B7ZFAaYK","colab_type":"text"},"cell_type":"markdown","source":["## Couche convolution #2\n","* **inputs** : 32 *feature maps* (= *input* de la couche précédente, *pooling*)\n","* Transformation : convolution 5 x 5 \n","* **output** : 64 *feature maps*\n","* activation *reLu*\n","* max-pool 2 x 2\n","\n","Lien de connection : *h_pool2*\n","\n","Important pour comprendre le détail des mécanismes de la convolution mais **à passer en 1ère lecture** :\n","\n","Dimension *h_pool2*  (tenseur)\n","```\n"," [batch_size, image_height, image_width, channels] =\n"," \n"," [-1, 7, 7, 32]\n","\n","```\n","car *max_pool_2x2* reduit la dimension (*down-sampling*) de 50% des images précédentes (taille 14 x 14)\n","\n","taille 14 x 14 -> max Pool -> taille 7 x 7"]},{"metadata":{"id":"LBmSqE_AwbsR","colab_type":"code","colab":{}},"cell_type":"code","source":["# input : output de la 1ère couche de convolution\n","channel = nb_features\n","# output de la 2ème couche de convolution:\n","nb_features = 64\n","W_conv2 = weight_variable([filter_size, filter_size, channel ,nb_features])\n","b_conv2 = bias_variable([nb_features])\n","h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n","h_pool2 = max_pool_2x2(h_conv2)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NXnyWBFNAmYx","colab_type":"text"},"cell_type":"markdown","source":["## Couche dense\n","Cette couche va se charger de l'analyse des \"structures\" (*feature maps*) émises par la dernière couche de convolution.\n","\n","C'est une couche dense (*fully connected*)"]},{"metadata":{"id":"2hA-c9tuweqE","colab_type":"code","colab":{}},"cell_type":"code","source":["# dim_out : 64 filtres taille 7 x 7 (2e couche de convolution)\n","dim_out = 7 * 7 * 64 \n","# nb neurones de la couche\n","nb_neurons = 1024\n","#dim_2 = nb_neurons\n","W_fc1 = weight_variable([dim_out, nb_neurons])\n","b_fc1 = bias_variable([nb_neurons])\n","\n","# flatten : tensor -> vector:\n","# mult matrice de poids W_fc1 x flatten vect (h_pool2_flat)\n","# puis addition biais (b_fc1) et application fonction activation ReLU \n","h_pool2_flat = tf.reshape(h_pool2, [-1, dim_out])\n","h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V_h1uwid-eyH","colab_type":"text"},"cell_type":"markdown","source":["## Couche finale\n","C'est la couche de décision. Celle qui effectue la classification\n","### Drop-out\n","La couche finale est **régularisée** afin d'augmenter la performance du modèle.\n","L'opération de **Drop-out** évite aux neurones de se sur-spécialiser.\n","\n","\n","Point technique important (passer en 1ère lecture):\n","Le *drop-out* ne concerne **que** la phase d'apprentissage.\n","Durant la phase de test, les neurones ne sont pas élagués. C'est pourquoi, durant cette phase, on indique :\n","\n","```keep_prob: 1.0\n","```\n","\n","Idem, durant les phases de reporting : pas de *droup-out*\n","\n","### softmax\n","Cette fonction émet une probabilité d'appartenance à chaque classe pour chaque image analysée."]},{"metadata":{"id":"CHnP9WeewygA","colab_type":"code","colab":{}},"cell_type":"code","source":["keep_prob = tf.placeholder(\"float\")\n","h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n","\n","W_fc2 = weight_variable([nb_neurons, num_classes])\n","b_fc2 = bias_variable([num_classes])\n","y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RYptNMGiA1Rr","colab_type":"text"},"cell_type":"markdown","source":["## Apprentissage\n","Les paramètres de l'apprentissage sont les suivants\n","* taux d'apprentissage (*learning_rate*)\n","* nb d'itérations total (*epochs*)\n","* taille des batchs (*batch_size*)"]},{"metadata":{"id":"yfqPie1Zx2XB","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.001\n","epochs = 2000 \n","batch_size = 128"],"execution_count":0,"outputs":[]},{"metadata":{"id":"StVK736gw3MY","colab_type":"code","colab":{}},"cell_type":"code","source":["# fonction de calcul de l'erreur\n","cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n","# optimizer\n","train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n","# prédiction correcte ?\n","correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n","# métrique de performance\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3tNoh7cZw5Yt","colab_type":"code","outputId":"762578c3-842b-40c5-ad9d-badf879d550b","executionInfo":{"status":"ok","timestamp":1553597873236,"user_tz":-60,"elapsed":36147,"user":{"displayName":"bardol franck","photoUrl":"https://lh4.googleusercontent.com/-JNlTS3TmFkQ/AAAAAAAAAAI/AAAAAAAAIEA/fJiA10muy2o/s64/photo.jpg","userId":"04651379096066137536"}},"colab":{"base_uri":"https://localhost:8080/","height":1054}},"cell_type":"code","source":["sess = tf.Session()\n","sess.run(tf.global_variables_initializer())\n","for i in range(epochs):\n","     batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n","     sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: dropout})\n","     if i % 100 == 0:\n","          train_accuracy = sess.run(accuracy, feed_dict={x: batch_xs, y_: batch_ys,\n","               keep_prob: 1.0})\n","          print(\"Step %d - Train accuracy %.3f\" % (i, train_accuracy))\n","                    \n","          test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels,\n","                                  keep_prob: 1.0})\n","          print(\"Step %d - Test accuracy %.3f\" % (i, test_accuracy))\n","          print(\"\")\n","          \n","test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels,\n","keep_prob: 1.0})\n","print(\"Test accuracy %.3f\" % test_accuracy) \n","\n","tf.logging.set_verbosity(old_v)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Step 0 - Train accuracy 0.133\n","Step 0 - Test accuracy 0.117\n","\n","Step 100 - Train accuracy 0.922\n","Step 100 - Test accuracy 0.955\n","\n","Step 200 - Train accuracy 0.992\n","Step 200 - Test accuracy 0.970\n","\n","Step 300 - Train accuracy 0.977\n","Step 300 - Test accuracy 0.978\n","\n","Step 400 - Train accuracy 0.969\n","Step 400 - Test accuracy 0.979\n","\n","Step 500 - Train accuracy 0.992\n","Step 500 - Test accuracy 0.983\n","\n","Step 600 - Train accuracy 0.992\n","Step 600 - Test accuracy 0.984\n","\n","Step 700 - Train accuracy 0.984\n","Step 700 - Test accuracy 0.984\n","\n","Step 800 - Train accuracy 0.984\n","Step 800 - Test accuracy 0.987\n","\n","Step 900 - Train accuracy 0.992\n","Step 900 - Test accuracy 0.988\n","\n","Step 1000 - Train accuracy 0.984\n","Step 1000 - Test accuracy 0.988\n","\n","Step 1100 - Train accuracy 0.992\n","Step 1100 - Test accuracy 0.989\n","\n","Step 1200 - Train accuracy 1.000\n","Step 1200 - Test accuracy 0.991\n","\n","Step 1300 - Train accuracy 0.992\n","Step 1300 - Test accuracy 0.990\n","\n","Step 1400 - Train accuracy 1.000\n","Step 1400 - Test accuracy 0.989\n","\n","Step 1500 - Train accuracy 1.000\n","Step 1500 - Test accuracy 0.990\n","\n","Step 1600 - Train accuracy 1.000\n","Step 1600 - Test accuracy 0.989\n","\n","Step 1700 - Train accuracy 1.000\n","Step 1700 - Test accuracy 0.990\n","\n","Step 1800 - Train accuracy 1.000\n","Step 1800 - Test accuracy 0.989\n","\n","Step 1900 - Train accuracy 1.000\n","Step 1900 - Test accuracy 0.990\n","\n","Test accuracy 0.990\n"],"name":"stdout"}]},{"metadata":{"id":"QYfA5UbxXW1u","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}